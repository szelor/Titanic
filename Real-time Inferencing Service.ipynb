{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.16.0 to work with agogemls\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TitanicAutoML version 4\n"
     ]
    }
   ],
   "source": [
    "model = ws.models['TitanicAutoML']\n",
    "print(model.name, 'version', model.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titanic_service folder created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_name = 'titanic_service'\n",
    "\n",
    "# Create a folder for the web service files\n",
    "experiment_folder = './' + folder_name\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "print(folder_name, 'folder created.')\n",
    "\n",
    "# Set path for scoring script\n",
    "script_file = os.path.join(experiment_folder,\"score_titanic.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./titanic_service/score_titanic.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_file\n",
    "import json\n",
    "import joblib\n",
    "import pandas\n",
    "import time\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Called when the service is loaded\n",
    "def init():\n",
    "    global model\n",
    "    # Get the path to the deployed model file and load it\n",
    "    model_path = Model.get_model_path('TitanicAutoML')\n",
    "    model = joblib.load(model_path)\n",
    "    \n",
    "    #Print statement for appinsights custom traces:\n",
    "    print (\"model initialized\" + time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "# Called when a request is received\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        # Get the input data as a pandas df\n",
    "        data = json.loads(raw_data)['data']\n",
    "        data = pandas.read_json(data, orient='records')\n",
    "\n",
    "        # Get a prediction from the model\n",
    "        predictions = model.predict(data)\n",
    "        # Get the corresponding classname for each prediction (0 or 1)\n",
    "        classnames = ['not-survived', 'survived']\n",
    "        predicted_classes = []\n",
    "        for prediction in predictions:\n",
    "            predicted_classes.append(classnames[prediction])\n",
    "        \n",
    "        # Log the input and output data to appinsights:\n",
    "        info = {\n",
    "            \"input\": raw_data,\n",
    "            \"output\": predictions.tolist()\n",
    "            }\n",
    "        # Return the predictions as JSON\n",
    "        return json.dumps(predicted_classes)\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        print (error + time.strftime(\"%H:%M:%S\"))\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name AzureML-AutoML\n",
      "Name AzureML-PyTorch-1.0-GPU\n",
      "Name AzureML-Scikit-learn-0.20.3\n",
      "Name AzureML-TensorFlow-1.12-CPU\n",
      "Name AzureML-PyTorch-1.2-GPU\n",
      "Name AzureML-TensorFlow-2.0-GPU\n",
      "Name AzureML-TensorFlow-2.0-CPU\n",
      "Name AzureML-Chainer-5.1.0-GPU\n",
      "Name AzureML-TensorFlow-1.13-CPU\n",
      "Name AzureML-Minimal\n",
      "Name AzureML-Chainer-5.1.0-CPU\n",
      "Name AzureML-PyTorch-1.4-GPU\n",
      "Name AzureML-PySpark-MmlSpark-0.15\n",
      "Name AzureML-PyTorch-1.3-CPU\n",
      "Name AzureML-PyTorch-1.1-GPU\n",
      "Name AzureML-TensorFlow-1.10-GPU\n",
      "Name AzureML-PyTorch-1.2-CPU\n",
      "Name AzureML-TensorFlow-1.13-GPU\n",
      "Name AzureML-Hyperdrive-ForecastDNN\n",
      "Name AzureML-TensorFlow-1.10-CPU\n",
      "Name AzureML-PyTorch-1.3-GPU\n",
      "Name AzureML-PyTorch-1.4-CPU\n",
      "Name AzureML-Tutorial\n",
      "Name AzureML-PyTorch-1.0-CPU\n",
      "Name AzureML-PyTorch-1.1-CPU\n",
      "Name AzureML-TensorFlow-1.12-GPU\n",
      "Name AzureML-VowpalWabbit-8.8.0\n",
      "Name AzureML-Designer-VowpalWabbit\n",
      "Name AzureML-TensorFlow-2.2-GPU\n",
      "Name AzureML-TensorFlow-2.2-CPU\n",
      "Name AzureML-PyTorch-1.6-CPU\n",
      "Name AzureML-PyTorch-1.6-GPU\n",
      "Name AzureML-Triton\n",
      "Name AzureML-TensorFlow-2.3-CPU\n",
      "Name AzureML-TensorFlow-2.3-GPU\n",
      "Name AzureML-Sidecar\n",
      "Name AzureML-Dask-CPU\n",
      "Name AzureML-Dask-GPU\n",
      "Name AzureML-TensorFlow-2.1-GPU\n",
      "Name AzureML-PyTorch-1.5-GPU\n",
      "Name AzureML-PyTorch-1.5-CPU\n",
      "Name AzureML-TensorFlow-2.1-CPU\n",
      "Name AzureML-AutoML-DNN-Vision-GPU\n",
      "Name AzureML-AutoML-DNN\n",
      "Name AzureML-AutoML-DNN-GPU\n",
      "Name AzureML-AutoML-GPU\n",
      "Name AzureML-Designer-Score\n",
      "Name AzureML-Designer-PyTorch-Train\n",
      "Name AzureML-Designer-IO\n",
      "Name AzureML-Designer-Transform\n",
      "Name AzureML-Designer-Recommender\n",
      "Name AzureML-Designer-CV\n",
      "Name AzureML-Designer-NLP\n",
      "Name AzureML-Designer-PyTorch\n",
      "Name AzureML-Designer-CV-Transform\n",
      "Name AzureML-Designer\n",
      "Name AzureML-Designer-R\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "envs = Environment.list(workspace=ws)\n",
    "for env in envs:\n",
    "    print(\"Name\",env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dependency info in ./titanic_service/titanic_env.yml\n",
      "channels:\n",
      "- anaconda\n",
      "- conda-forge\n",
      "- pytorch\n",
      "dependencies:\n",
      "- python=3.6.2\n",
      "- pip:\n",
      "  - azureml-core==1.16.0\n",
      "  - azureml-pipeline-core==1.16.0\n",
      "  - azureml-telemetry==1.16.0\n",
      "  - azureml-defaults==1.16.0\n",
      "  - azureml-interpret==1.16.0\n",
      "  - azureml-automl-core==1.16.0\n",
      "  - azureml-automl-runtime==1.16.0\n",
      "  - azureml-train-automl-client==1.16.0\n",
      "  - azureml-train-automl-runtime==1.16.0\n",
      "  - azureml-dataset-runtime==1.16.0\n",
      "  - inference-schema\n",
      "  - py-cpuinfo==5.0.0\n",
      "- numpy~=1.18.0\n",
      "- scikit-learn==0.22.1\n",
      "- pandas~=0.25.0\n",
      "- py-xgboost<=0.90\n",
      "- fbprophet==0.5\n",
      "- holidays==0.9.11\n",
      "- setuptools-git\n",
      "- psutil>5.0.0,<6.0.0\n",
      "name: azureml_912ebce86aa851a4f789bc2c01e320a3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "\n",
    "env = Environment.get(ws, \"AzureML-AutoML\")\n",
    "\n",
    "env_file = os.path.join(experiment_folder,\"titanic_env.yml\")\n",
    "with open(env_file,\"w\") as f:\n",
    "    f.write(env.python.conda_dependencies.serialize_to_string())\n",
    "print(\"Saved dependency info in\", env_file)\n",
    "\n",
    "# Print the .yml file\n",
    "with open(env_file,\"r\") as f:\n",
    "    print(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running................................................\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core import Model\n",
    "\n",
    "# Configure the scoring environment\n",
    "inference_config = InferenceConfig(runtime= \"python\",\n",
    "                                   entry_script=script_file,\n",
    "                                   conda_file=env_file)\n",
    "\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
    "\n",
    "service_name = \"titanic\"\n",
    "\n",
    "service = Model.deploy(ws, service_name, [model], inference_config, deployment_config)\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titanic\n"
     ]
    }
   ],
   "source": [
    "for webservice_name in ws.webservices:\n",
    "    print(webservice_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.update(enable_app_insights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not-survived\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas\n",
    "\n",
    "test_df = pandas.read_csv('test.csv')\n",
    "test =test_df.drop(['PassengerId'], axis=1)\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "test_json = test[0:1].to_json()\n",
    "input_json = json.dumps({\"data\": test_json})\n",
    "\n",
    "# Call the web service, passing the input data (the web service will also accept the data in binary format)\n",
    "predictions = service.run(input_data = input_json)\n",
    "\n",
    "# Get the predicted class - it'll be the first (and only) one.\n",
    "predicted_classes = json.loads(predictions)\n",
    "print(predicted_classes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-25T11:14:38,906121137+00:00 - iot-server/run \n",
      "2020-10-25T11:14:38,907671263+00:00 - gunicorn/run \n",
      "2020-10-25T11:14:38,918898557+00:00 - rsyslog/run \n",
      "2020-10-25T11:14:38,932286187+00:00 - nginx/run \n",
      "/usr/sbin/nginx: /azureml-envs/azureml_912ebce86aa851a4f789bc2c01e320a3/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_912ebce86aa851a4f789bc2c01e320a3/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_912ebce86aa851a4f789bc2c01e320a3/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_912ebce86aa851a4f789bc2c01e320a3/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_912ebce86aa851a4f789bc2c01e320a3/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "rsyslogd: /azureml-envs/azureml_912ebce86aa851a4f789bc2c01e320a3/lib/libuuid.so.1: no version information available (required by rsyslogd)\n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2020-10-25T11:14:39,178843933+00:00 - iot-server/finish 1 0\n",
      "2020-10-25T11:14:39,180419660+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 19.9.0\n",
      "Listening at: http://127.0.0.1:31311 (12)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 39\n",
      "SPARK_HOME not set. Skipping PySpark Initialization.\n",
      "Generating new fontManager, this may take some time...\n",
      "Initializing logger\n",
      "2020-10-25 11:14:42,453 | root | INFO | Starting up app insights client\n",
      "Starting up app insights client\n",
      "2020-10-25 11:14:42,454 | root | INFO | Starting up request id generator\n",
      "Starting up request id generator\n",
      "2020-10-25 11:14:42,454 | root | INFO | Starting up app insight hooks\n",
      "Starting up app insight hooks\n",
      "2020-10-25 11:14:42,454 | root | INFO | Invoking user's init function\n",
      "Invoking user's init function\n",
      "2020-10-25 11:14:43,521 | root | INFO | Users's init has completed successfully\n",
      "Users's init has completed successfully\n",
      "2020-10-25 11:14:43,525 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n",
      "Skipping middleware: dbg_model_info as it's not enabled.\n",
      "2020-10-25 11:14:43,525 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "2020-10-25 11:14:43,527 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n",
      "Scoring timeout is found from os.environ: 60000 ms\n",
      "2020-10-25 11:15:06,304 | root | INFO | Swagger file not present\n",
      "Swagger file not present\n",
      "2020-10-25 11:15:06,305 | root | INFO | 404\n",
      "404\n",
      "127.0.0.1 - - [25/Oct/2020:11:15:06 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2020-10-25 11:15:08,126 | root | INFO | Swagger file not present\n",
      "Swagger file not present\n",
      "2020-10-25 11:15:08,127 | root | INFO | 404\n",
      "404\n",
      "127.0.0.1 - - [25/Oct/2020:11:15:08 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2020-10-25 11:15:29,987 | root | INFO | Validation Request Content-Type\n",
      "Validation Request Content-Type\n",
      "2020-10-25 11:15:29,987 | root | INFO | \tHost: localhost:5001\n",
      "\tHost: localhost:5001\n",
      "2020-10-25 11:15:29,988 | root | INFO | \tX-Real-Ip: 127.0.0.1\n",
      "\tX-Real-Ip: 127.0.0.1\n",
      "2020-10-25 11:15:29,988 | root | INFO | \tX-Forwarded-For: 127.0.0.1\n",
      "\tX-Forwarded-For: 127.0.0.1\n",
      "2020-10-25 11:15:29,988 | root | INFO | \tX-Forwarded-Proto: http\n",
      "\tX-Forwarded-Proto: http\n",
      "2020-10-25 11:15:29,988 | root | INFO | \tConnection: close\n",
      "\tConnection: close\n",
      "2020-10-25 11:15:29,988 | root | INFO | \tContent-Length: 262\n",
      "\tContent-Length: 262\n",
      "2020-10-25 11:15:29,988 | root | INFO | \tUser-Agent: python-requests/2.24.0\n",
      "\tUser-Agent: python-requests/2.24.0\n",
      "2020-10-25 11:15:29,988 | root | INFO | \tAccept: application/json\n",
      "\tAccept: application/json\n",
      "2020-10-25 11:15:29,988 | root | INFO | \tAccept-Encoding: gzip, deflate\n",
      "\tAccept-Encoding: gzip, deflate\n",
      "2020-10-25 11:15:29,988 | root | INFO | \tContent-Type: application/json\n",
      "\tContent-Type: application/json\n",
      "2020-10-25 11:15:29,988 | root | INFO | \tX-Ms-Request-Id: a67cd8b6-da9a-4899-8e64-d16913c3f144\n",
      "\tX-Ms-Request-Id: a67cd8b6-da9a-4899-8e64-d16913c3f144\n",
      "2020-10-25 11:15:29,988 | root | INFO | Scoring Timer is set to 60.0 seconds\n",
      "Scoring Timer is set to 60.0 seconds\n",
      "2020-10-25 11:15:30,054 | root | INFO | 200\n",
      "200\n",
      "127.0.0.1 - - [25/Oct/2020:11:15:30 +0000] \"POST /score HTTP/1.0\" 200 20 \"-\" \"python-requests/2.24.0\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://8fa1f181-3eac-4392-aca9-3bd35c3209ea.westus.azurecontainer.io/score\n"
     ]
    }
   ],
   "source": [
    "endpoint = service.scoring_uri\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not-survived', 'not-survived', 'not-survived', 'not-survived', 'survived']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "test_json = test[0:5].to_json()\n",
    "input_json = json.dumps({\"data\": test_json})\n",
    "\n",
    "# Set the content type\n",
    "headers = { 'Content-Type':'application/json' }\n",
    "\n",
    "predictions = requests.post(endpoint, input_json, headers = headers)\n",
    "predicted_classes = json.loads(predictions.json())\n",
    "\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service deleted.\n"
     ]
    }
   ],
   "source": [
    "service.delete()\n",
    "print ('Service deleted.')"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{"84c2b86d-9781-41b0-91cd-80ffb632dcf1_2":{"accuracy":[0.7822721598002497],"weighted_accuracy":[0.7993546876147485],"AUC_weighted":[0.8223131948829036],"recall_score_micro":[0.7822721598002497],"average_precision_score_micro":[0.8249884026874194],"AUC_macro":[0.8223131948829037],"precision_score_weighted":[0.7861474438916974],"f1_score_micro":[0.7822721598002496],"f1_score_macro":[0.7635483547071399],"recall_score_weighted":[0.7822721598002497],"average_precision_score_weighted":[0.8256165568996728],"f1_score_weighted":[0.7807400429725815],"matthews_correlation":[0.5344694965636065],"AUC_micro":[0.8378251717188719],"precision_score_macro":[0.7722042412271924],"average_precision_score_macro":[0.8100605958404892],"recall_score_macro":[0.7626909895508467],"log_loss":[0.5067042450461313],"norm_macro_recall":[0.5253819791016934],"balanced_accuracy":[0.7626909895508467],"precision_score_micro":[0.7822721598002497]},"84c2b86d-9781-41b0-91cd-80ffb632dcf1_1":{"AUC_weighted":[0.8616129428134338],"average_precision_score_micro":[0.8569943727187834],"precision_score_macro":[0.8026064898350842],"norm_macro_recall":[0.5855835176458593],"matthews_correlation":[0.5951727078936224],"weighted_accuracy":[0.8272922308943723],"log_loss":[0.44335843178563683],"AUC_micro":[0.8739025702890114],"AUC_macro":[0.8616129428134338],"average_precision_score_weighted":[0.8632121333261631],"f1_score_macro":[0.7948636533379357],"recall_score_weighted":[0.8114357053682895],"precision_score_micro":[0.8114357053682895],"recall_score_macro":[0.7927917588229296],"precision_score_weighted":[0.8136736476157264],"recall_score_micro":[0.8114357053682895],"balanced_accuracy":[0.7927917588229296],"accuracy":[0.8114357053682895],"f1_score_weighted":[0.8098843688450206],"f1_score_micro":[0.8114357053682897],"average_precision_score_macro":[0.8550873806975142]},"84c2b86d-9781-41b0-91cd-80ffb632dcf1_0":{"recall_score_macro":[0.8016294806805867],"balanced_accuracy":[0.8016294806805867],"average_precision_score_macro":[0.8498066589599356],"precision_score_macro":[0.8151962497100197],"average_precision_score_micro":[0.8564750480554808],"average_precision_score_weighted":[0.8597109335499493],"AUC_micro":[0.8736213394305807],"norm_macro_recall":[0.6032589613611733],"f1_score_macro":[0.8062247829157666],"precision_score_weighted":[0.8226627383643459],"matthews_correlation":[0.6166158426560109],"recall_score_micro":[0.8226342072409487],"f1_score_micro":[0.8226342072409487],"AUC_macro":[0.8596816567387477],"accuracy":[0.8226342072409487],"f1_score_weighted":[0.8206411061821782],"weighted_accuracy":[0.8401804293095866],"precision_score_micro":[0.8226342072409487],"recall_score_weighted":[0.8226342072409487],"AUC_weighted":[0.8596816567387477],"log_loss":[0.5415331291087221]},"84c2b86d-9781-41b0-91cd-80ffb632dcf1_3":{"precision_score_macro":[0.819148959478604],"norm_macro_recall":[0.6081557318130465],"f1_score_micro":[0.8249063670411985],"accuracy":[0.8249063670411985],"average_precision_score_weighted":[0.8642199739401228],"log_loss":[0.438200474530878],"AUC_weighted":[0.8642864902136613],"matthews_correlation":[0.6228782368097576],"AUC_micro":[0.877890849609025],"recall_score_micro":[0.8249063670411985],"f1_score_macro":[0.8077676792927777],"f1_score_weighted":[0.8226955254357697],"precision_score_weighted":[0.8276450855067768],"balanced_accuracy":[0.8040778659065232],"average_precision_score_micro":[0.8592012705193633],"recall_score_weighted":[0.8249063670411985],"recall_score_macro":[0.8040778659065232],"weighted_accuracy":[0.8425669320329565],"AUC_macro":[0.8642864902136616],"average_precision_score_macro":[0.85599072973135],"precision_score_micro":[0.8249063670411985]},"84c2b86d-9781-41b0-91cd-80ffb632dcf1_4":{"weighted_accuracy":[0.8450896226748789],"average_precision_score_weighted":[0.862544784034079],"AUC_weighted":[0.8631395666738195],"precision_score_micro":[0.8271535580524345],"recall_score_macro":[0.8059370898221989],"f1_score_macro":[0.8101304607121632],"balanced_accuracy":[0.8059370898221989],"accuracy":[0.8271535580524345],"norm_macro_recall":[0.6118741796443976],"matthews_correlation":[0.6268727769822149],"recall_score_micro":[0.8271535580524345],"AUC_macro":[0.8631395666738193],"f1_score_weighted":[0.8249258016748561],"log_loss":[0.43234631753403097],"precision_score_weighted":[0.8291227836115889],"AUC_micro":[0.8743940252586887],"recall_score_weighted":[0.8271535580524345],"average_precision_score_micro":[0.8531383868295661],"precision_score_macro":[0.8212572872147579],"average_precision_score_macro":[0.854011865942678],"f1_score_micro":[0.8271535580524343]}}